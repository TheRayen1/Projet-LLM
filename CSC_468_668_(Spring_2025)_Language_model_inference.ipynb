{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheRayen1/Projet-LLM/blob/main/CSC_468_668_(Spring_2025)_Language_model_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language model inference and few-shot prompting\n",
        "\n",
        "[Mark J. Nelson](https://www.kmjn.org/), Spring 2025\n",
        "\n",
        "This notebook demonstrates inference with pre-trained large language models (LLMs). \"Inference\" is neural network jargon for *using* a trained model, versus training it. For example, if you have an image classifier, inference is using it to classify images. With large language models, inference means generating text.\n",
        "\n",
        "For now we will stick to \"base\" language models that are trained solely on the \"autoregressive objective\", i.e. predicting the next word. Even these language models start to exhibit things that look sort of like problem-solving or question-answering once they reach a certain size, but they are not explicitly trained to solve problems or answer questions in the way models like ChatGPT are.\n",
        "\n",
        "In my opinion it's important to build intuition with LLMs by starting with base language models that have only been trained to model language, and haven't been further trained with another objective on top of the language modeling objective. Once you understand these base models, it provides a good foundation for understanding the models that have undergone other types of training on top of that.\n"
      ],
      "metadata": {
        "id": "hHyGmxAj89pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "eft9MafhEXBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First check that we have a GPU attached to this notebook:"
      ],
      "metadata": {
        "id": "m6EGXY3d9msi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJlQzC1EGLGk",
        "outputId": "d05231f1-46b5-4217-bff4-6a5928d204f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar  4 02:50:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a language model"
      ],
      "metadata": {
        "id": "h-S-m5FmEZ3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to download and load an LLM.\n",
        "\n",
        "We will use the [`transformers`](https://huggingface.co/docs/transformers/en/index) library from Huggingface for inference. There are many libraries for LLM inference, but `transformers` is one of the more popular that supports many open-source language models. For more industrial-strength usage (e.g. serving clients), [vLLM](https://docs.vllm.ai/) is another good option.\n",
        "\n",
        "We will use the [OLMo series of open-source models](https://allenai.org/olmo) from the Allen Institute for AI, initially [the \"1b\" base model](https://huggingface.co/allenai/OLMo-1B-hf).\n",
        "\n",
        "The model's size is how many parameters (neural network weights) it has, and 1b means 1 billion parameters. The OLMo models use 32-bit parameters, so RAM usage in bytes will be about 4x the number of parameters (32 bits equals 4 bytes). For example, the 1b parameter model will take about 4 GB of RAM. OLMo models also support 8-bit quantized versions, which will take 1/4 as much RAM for the same number of parameters. For example, if you load the 1b model in 8-bit quantized form, it will take about 1 GB of RAM.\n",
        "\n",
        "The amount of RAM we have available (both regular RAM and GPU vRAM) will often be a constraint in the size of models we can use. The Colab free tier (as of November 2024) provides 12.7 GB of regular RAM and a T4 GPU with 16 GB of vRAM."
      ],
      "metadata": {
        "id": "Dp2lAstQ964L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelname = \"allenai/OLMo-1B-hf\"      # the model to load, using its Huggingface repository name\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(modelname).to('cuda')\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelname)"
      ],
      "metadata": {
        "id": "HisNXdKCJFZJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "c822e847-e015-475f-8038-2e233eb50bd4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "THPDtypeType.tp_dict == nullptr INTERNAL ASSERT FAILED at \"../torch/csrc/Dtype.cpp\":176, please report a bug to PyTorch. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-21b0ddcf823c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodelname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"allenai/OLMo-1B-hf\"\u001b[0m      \u001b[0;31m# the model to load, using its Huggingface repository name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackbone_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackboneConfigMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackboneMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocstringParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeHintParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_json_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m from .doc import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: THPDtypeType.tp_dict == nullptr INTERNAL ASSERT FAILED at \"../torch/csrc/Dtype.cpp\":176, please report a bug to PyTorch. "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling from the model"
      ],
      "metadata": {
        "id": "FQeKqr4mEqfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model is loaded, we can use it. To generate text, we feed it a *prompt* that will be autocompleted. We will write a small helper function that given a prompt and a number *n*, generates *n* completions of the prompt and prints them.\n",
        "\n",
        "My code here explicitly tokenizes and de-tokenizes the prompt and the generated text to be clear about what's going on and let us inspect intermediate results if we want, but there is a simpler [pipeline API](https://huggingface.co/docs/transformers.js/en/pipelines) that does that for you if you just want to generate text.\n",
        "\n",
        "There are various things you can change here. For example, *temperature* controls how much variation there is in the generation process (lower temperature is more deterministic), and *max_new_tokens* controls how much text is generated."
      ],
      "metadata": {
        "id": "cOwYHo87_P9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 1.0\n",
        "max_new_tokens = 50\n",
        "\n",
        "def sample_llm(prompt, n):\n",
        "  # tokenize our prompt\n",
        "  tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "  # generate new text! This is where the magic happens\n",
        "  outputs = model.generate(**tokenized_prompt,\n",
        "                           do_sample=True,\n",
        "                           temperature=temperature,\n",
        "                           max_new_tokens=max_new_tokens,\n",
        "                           num_return_sequences=n)\n",
        "\n",
        "  # output by default includes BOTH our prompt and the generated text\n",
        "  # chop off our prompt from the beginning of each result so we can distinguish\n",
        "  outputs = outputs[:, len(tokenized_prompt['input_ids'][0]):]\n",
        "\n",
        "  # the output is a list of numerical tokens - decode it back to readable characters\n",
        "  outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "  # print the outputs\n",
        "  print()\n",
        "  for output in outputs:\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {output!r}\")"
      ],
      "metadata": {
        "id": "fgF8rh71J4YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference examples\n",
        "\n",
        "Now we're ready to generate some text!"
      ],
      "metadata": {
        "id": "5AIZDSDr_7ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text autocompletion\n",
        "\n",
        "The most straightforward type of language model inference is to give it the start of a sentence as prompt, and have it complete the sentence.\n",
        "\n",
        "(Technical note: Do *not* include a space at the end of the prompt. Due to the way most LLM tokenizers currently work, you will generally get much worse results. But you can try to see what happens!)"
      ],
      "metadata": {
        "id": "tseOuVBXEzSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_llm(\"A good definition of artificial intelligence is\", 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQK-sV-8ABNq",
        "outputId": "089bd38a-e41e-4f07-8480-da4adfd94917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: 'A good definition of artificial intelligence is', Generated text: ' the ability for a computer, or machine, to interact with people intelligently and autonomously. It means that computers have the ability to think and learn and adapt; it means learning in its purest form.\\nComputer scientists define artificial intelligence as'\n",
            "Prompt: 'A good definition of artificial intelligence is', Generated text: ' given by Thomas J. Watson founder of IBM. Watson is a phrase coined from the Greek word for clever (xanthe) and the Latin word for intelligence (artem). Watson is the machine that uses its own unique intelligence (artificial)'\n",
            "Prompt: 'A good definition of artificial intelligence is', Generated text: ' to put into a machine the processes required to learn from experience without requiring training, except by using a method called reinforcement learning. Reinforcement learning is the practice of a computer algorithm learning to play a new game or solve a new problem from experience.\\n'\n",
            "Prompt: 'A good definition of artificial intelligence is', Generated text: ' the intelligence displayed by machines that we recognize, learn, react to, communicate with, and perceive. If you haven’t read The Rise of the machine intelligence, you might want to check that out.\\nArtificial Intelligence is also sometimes referred to'\n",
            "Prompt: 'A good definition of artificial intelligence is', Generated text: ' not only about learning from information and algorithms and taking action based on that knowledge but also thinking independently and creatively to solve problems, to build knowledge and to learn from mistakes.\" - Aya Alzahir.\\n\"The aim of artificial intelligence'\n",
            "Prompt: 'A good definition of artificial intelligence is', Generated text: ' the way that human and autonomous entities interact with each other as part of a complex system [28].\\nArtificial intelligence is a specific type of technology that creates and uses computer programs with the ultimate objective of making human intelligence more efficient. Thus, artificial'\n",
            "Prompt: 'A good definition of artificial intelligence is', Generated text: ' that while it can be defined in many ways, it refers to any program the human mind can generate, whether the goal is automated speech translation or an intelligent chatbot.\\nHowever, this was not always the case with Artificial Intelligence, which was'\n",
            "Prompt: 'A good definition of artificial intelligence is', Generated text: ' machine learning that can make better and more intelligent decisions on your behalf.\\nHow Do Companies Benefit From Artificial Intelligence?\\nAn important thing to understand about artificial intelligence is that it requires much less work from your part.\\nArtificial intelligence means'\n",
            "Prompt: 'A good definition of artificial intelligence is', Generated text: \" the ability of computers to perform tasks that require human intelligence.\\nToday's computers are extremely intelligent; they can perform complex calculations, interpret pictures and movies, speak the internet, and more.\\nHowever, at some point in the near future, our\"\n",
            "Prompt: 'A good definition of artificial intelligence is', Generated text: ' intelligent machines that can self-direct, self-regulate, learn and change and use the information they learn to perform activities that would, ideally, humans would do better. In this context, artificial intelligence is distinguished from machine learning, which refers to data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_llm(\"Good morning\", 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaRKu7E2BG_M",
        "outputId": "2b36781c-4d24-4aec-92f0-e10aa58d7dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: 'Good morning', Generated text: ', good afternoon and welcome to our conference call with the financial results for the fourth quarter 2013.\\nJoining me today are Mr. Michael L. Rachlin, Chief Executive Officer & President; Mr. Robert Rathgeb, Interim Chief'\n",
            "Prompt: 'Good morning', Generated text: '. We are working on adding a link to a post about the story of Esther to the main menu of the site, so please look for that tomorrow. The story is pretty interesting, if a little scary. I think one of the funniest'\n",
            "Prompt: 'Good morning', Generated text: '.” She smiles.\\nA few of the people in the class, including yours truly, are still in their sleep. Some of the others are awake. I’m the last person awake and have to get up fast before all of my fellow sleep'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question-answering"
      ],
      "metadata": {
        "id": "mDKiIUwOD2D7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These base LLMs are trained only to generate language, not to \"chat\" or respond to commands. So directly asking questions doesn't produce great results:"
      ],
      "metadata": {
        "id": "QGyBcpFG3fBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_llm(\"What is the color of the ocean?\", 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twqdHgrN3R2o",
        "outputId": "999d1d40-4219-4df0-b708-445211b66fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: 'What is the color of the ocean?', Generated text: \" What's the color of that color? That is not an ocean. It must be something very different. The ocean must be something very different.\\nThe ocean is just water. It is a very beautiful, blue, beautiful, blue water. It\"\n",
            "Prompt: 'What is the color of the ocean?', Generated text: '\\nThe colour of ocean water depends on many conditions. It can be white or blue because water evaporates quickly. The colour of the ocean changes depending on air pressure and temperature.\\nThe colour of the ocean depends on the amount of sunlight absorbed by'\n",
            "Prompt: 'What is the color of the ocean?', Generated text: '\\nIt is the color of the sky, water, the moon, and the sun.\\nWhat would you need to go to heaven?\\nA white bible and a cross.\\nWhat is the hardest animal?\\nA dog in boots.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nonetheless, we can get base LLMs to perform some tasks by *prompt engineering*. Specifically, we need to design a prompt so that if the model *autocompletes* the prompt (since autocompleting is all it does), it will produce something in the format we want.\n",
        "\n",
        "One possibility is to write a prompt that looks like a Q&A format:"
      ],
      "metadata": {
        "id": "lSnala5y3mmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_llm(\"Question: What is the color of the ocean? Answer:\", 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfH8b5OmELFF",
        "outputId": "ef67ce2f-7ae7-4b99-831a-1864068e90cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: 'Question: What is the color of the ocean? Answer:', Generated text: ' Blue.\\nIs the ocean blue? Let’s explore this with the help of some photos.\\nA: I’ll choose green; a pretty vibrant color, right?\\nA: That’s right. In fact, it’s'\n",
            "Prompt: 'Question: What is the color of the ocean? Answer:', Generated text: ' Blue. What is the distance between the earth and the sun? Answer: 365,000 miles. What is the largest living organism? Answer: An orange.\\nHow do you make a baby dolls eyes?\\nHow do you stop a person'\n",
            "Prompt: 'Question: What is the color of the ocean? Answer:', Generated text: ' Ocean. The color of the sea is blue and green, and it is made up of light and dark shades of blue and green.\\nQuestion: Where can I find cheap hotel deals in the US? Answer: Hotels, airlines, and other'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or to give it something that looks like it's supposed to complete an analogy:"
      ],
      "metadata": {
        "id": "SEiGmx3hFA5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_llm(\"Bird is to flight as cheetah is to\", 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhhqQ8P_FFA-",
        "outputId": "b0f4fe7d-3b1b-417b-ae03-9f22599b94a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: 'Bird is to flight as cheetah is to', Generated text: ' speed, and in this case the cheetah is the bird.\\nThe flightless bird is a symbol of the powerful force of the water, which is symbolized by the waterfall.The Rival series of golf balls are a mid'\n",
            "Prompt: 'Bird is to flight as cheetah is to', Generated text: ' speed. He flies faster than the speed at which you can run, making it more like a blur than a speed, though I would add this: If you were aiming for high fives, you’d know not to try and do it while'\n",
            "Prompt: 'Bird is to flight as cheetah is to', Generated text: ' speed.\\nBut the most effective hunters, the most cunning hunters, can outwit, outrun, and out-muscle a predator with a single glance. These animals can run up to half a mile without tiring, and can dive'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or the start of a list:"
      ],
      "metadata": {
        "id": "dI7ql3KRI1m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_llm(\"1. London, 2. Paris, 3.\", 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC1r9wo_I3vI",
        "outputId": "49488536-9381-4d2a-fa71-3637f4d8c0e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: '1. London, 2. Paris, 3.', Generated text: ' New York. In total, 5.3 million, according to Géo.\\n\"We estimate that over the past 14 years, France (and particularly the Paris region) has created two generations of residents. The second generation is a little less'\n",
            "Prompt: '1. London, 2. Paris, 3.', Generated text: ' New York, 4. San Francisco.\\nI didn’t even know the UK was on the list, so I didn’t have to go around guessing where the world was.\\nAnyway, I knew that I wanted London as my first stop'\n",
            "Prompt: '1. London, 2. Paris, 3.', Generated text: ' New-York, for a few years past.\\nHis works are of the most extraordinary excellence, they are of uncommon variety, and have a very extraordinary effect in almost all their kinds. What appears of most importance, is the great variety, so'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few-shot prompting\n",
        "\n"
      ],
      "metadata": {
        "id": "6th27jWsE5Vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might notice that the model doesn't do very well at these question-answering or analogy tasks, especially if you're using a small model. The way we've phrased the tasks above is called *zero-shot* question answering. We can improve performance by giving the model some examples of what we want as answers, and *then* ask it to autocomplete. The number of examples we give it is the number of \"shots\", e.g. if we give it one example, that's called 1-shot question answering.\n",
        "\n",
        "The property that models are (sometimes) able to generalize from a few examples to greatly improve performance is called *in-context learning*."
      ],
      "metadata": {
        "id": "ZkIWeqv1FLyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_llm(\"Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:\", 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXM_WmXwFp4F",
        "outputId": "adadc0d4-bee2-4c9a-8eac-9a28be48544e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: 'Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:', Generated text: ' Blue Q: What’s the color of money? A: Money is green Q: What makes a rainbow? A: Rain Q: What color is a flower? A: Pink Q: What color does your dog wear to a party? A'\n",
            "Prompt: 'Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:', Generated text: ' Sea green.\\nQ: What is a light-colored, grassy shrub? A: A holly. Q: What flower is also called a holly? A: Holly berry. Q: In the middle of a garden,'\n",
            "Prompt: 'Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:', Generated text: ' Gray. Q: Where is the color white?\\n5. What is the color of snow? A: Gray. Q: What color is blood? A: Red. A: Red. Q: What color is black? A: Black.'\n",
            "Prompt: 'Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:', Generated text: ' Blue. Q: Where is the color red? A: Red.\\nS.C.P. : When a book is written, it is a book with a cover and a page. a. A BOOK is A. BOOK is'\n",
            "Prompt: 'Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:', Generated text: ' Blue. Question 12. When planting flowers, which parts of the plant do you plant first, stem or leaf? A: Leaf. Question 13. If the sun was directly overhead and shining on three different leaves in a plant, what do all the'\n",
            "Prompt: 'Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:', Generated text: ' Yellow.\\nIt’s the small differences that can get you through the day.The Department in charge of the implementation of all State policies and activities related to the prevention and control of tobacco use, in particular the introduction of new tobacco regulatory'\n",
            "Prompt: 'Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:', Generated text: ' Blue Q: What is the color of a rainbow? A: Red-red-red-green-green-red. Q: What is the color of rainbows that have lasted more than 14 seconds? A: Green-blue-green.'\n",
            "Prompt: 'Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:', Generated text: ' Grey. Q: What is the color of a room with windows to the east and west? A: Gray. Q: What happens if two rooms have windows facing east and west? A: A gray color is formed.\\nColor theory is one'\n",
            "Prompt: 'Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:', Generated text: \" Blue.... But the truth is that blue is just an accent color and can really just be used to add a pop of color to other neutral shades. When you've reached your favorite light blue nail polish, be sure to add a little bit of\"\n",
            "Prompt: 'Q: What is the color of a lawn? A: Green. Q: What is the color of the ocean? A:', Generated text: ' Blue Q: What is the color of the sky at sunset? A: Sky?\\nA: A rainbow Q: What is the color of your body? A: Red. Q: What is the color of the sky on a good day?'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, giving the model one correct example of answering a color question now makes it able to reliably tell us that the color of the ocean is blue, while it only got that answer right some of the time in the zero-shot case further above. Providing an example of a correct answer also reduces the variability of the *form* of the responses: it now always gives a single-word answer (the name of a color), while in the zero-shot case it sometimes gave one-word answers, and other times answers in the form of a sentence like \"The color is blue.\".\n",
        "\n",
        "Few-shot prompting doesn't always improve performance though! Here's an example where performance gets worse:"
      ],
      "metadata": {
        "id": "IUnKNDJ4KlfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_llm(\"Big is to small as old is to new. Bird is to flight as cheetah is to\", 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlXdhsEIGg_x",
        "outputId": "26117f98-e0cb-4960-8b2e-6c42fcb402bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: 'Big is to small as old is to new. Bird is to flight as cheetah is to', Generated text: ' leopardo.\\nWoah! How’s this? The ‘Cheetah’ is the king when it comes to moving fast but with great strength. There is no doubt which is the king of speed! A leopard that moves'\n",
            "Prompt: 'Big is to small as old is to new. Bird is to flight as cheetah is to', Generated text: ' speed.\\nFriends in India are like sunsets-there are a lot of them, but one is more beautiful than the other.\\nThe sun is like the earth-it is round, even, and beautiful.\\nWe don’t'\n",
            "Prompt: 'Big is to small as old is to new. Bird is to flight as cheetah is to', Generated text: ' cat. Cheetahs are to cheetahs and cheetahs are to cheetahs. All cheetahs are cheetahs and all cheetahs are cheetahs.Cherub'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Food for thought: What happened here?"
      ],
      "metadata": {
        "id": "JkXsAux6PoFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_llm(\"1. Germany, 2. France, 3. Poland. 1. Japan, 2. China, 3.\", 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DGwfFLcCiHF",
        "outputId": "fc4a9dd5-9bff-4630-f74f-b46e5b428f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: '1. Germany, 2. France, 3. Poland. 1. Japan, 2. China, 3.', Generated text: ' Hungary. 1. Japan, 2. Hungary. The top table shows Japan and their scores. 1. Turkey. 2. Japan. 3. Hungary. It is the national table with the current world ranking. 1. Spain, 2. Holland.'\n",
            "Prompt: '1. Germany, 2. France, 3. Poland. 1. Japan, 2. China, 3.', Generated text: ' South Korea, 2. Russia.\\n2. China, 3. Germany, 4. USA.\\n2. China, 3. USA, 4. Russia.\\n2. Russia, 3. USA, 1. China.\\n3. USA'\n",
            "Prompt: '1. Germany, 2. France, 3. Poland. 1. Japan, 2. China, 3.', Generated text: ' Russia. 1. Australia, 2. Canada. Germany, France, Great Britain, Japan, Russia, China, Sweden, Italy, Spain, etc.\\nAnd so they come out in the top list of the world.\\nThis doesn’t'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "Your assignment is to investigate how three factors impact question-answering performance of base LLMs, while holding the model fixed:\n",
        "\n",
        "* The question being answered\n",
        "* The number of examples given in the prompt (n-shot learning)\n",
        "* The exact wording of the prompt\n",
        "\n",
        "Do the following:\n",
        "\n",
        "1. Invent two problems similar to the ones above. They can be explicit Q&A, an analogy problem, or something else. Try to make one easy and one harder. It should be something where you can tell when an answer is correct, so try not to make the questions too subjective.\n",
        "\n",
        "2. Test the model's zero-shot performance on the two problems. Generate 10 responses and count how many times the LLM gets the answer right. Think carefully about what counts as a right answer!\n",
        "\n",
        "3. Now try with a 1-shot prompt, where you give one example of a correct question/answer to a similar (but not identical) question in the prompt, before asking it the same question as in #2. How does adding one example impact performance, compared to the 0-shot performance?\n",
        "\n",
        "4. Finally, make some minor changes to the wording of a few of your examples in #3 (\"the color is\" -> \"has the color\", for example). How does this change performance? Does accuracy decrease if the example correct answer you give in the 1-shot prompt uses slightly different wording from the question the LLM has to answer?"
      ],
      "metadata": {
        "id": "Cuswl7pZ7CVQ"
      }
    }
  ]
}